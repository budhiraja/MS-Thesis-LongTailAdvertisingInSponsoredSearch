In this chapter, we discuss some of key papers related to sponsored search, followed by the relevant work done towards long tail queries and coverage patterns.

\section{Sponsored Search}

The most important research issue in sponsored search is the relevance of an ad for a given query. In the past, attempts have been made to use the traditional IR approaches to improve ad query relevance for sponsored search \cite{raghavan2008evaluating}. The authors in \cite{raghavan2008evaluating} compared traditional methods - BM25, modified TF-IDF, language models and translation models for ad-query relevance. The authors further modified the translation models by augmenting it with the probability that the ad will be clicked for a query-ad pair through a mixture model. In another study \cite{hillard2010improving}, the authors modelled ad-query relevance as a machine learning problem. The authors proposed to use 19 initial features including common n-grams between ad copy and query and cosine similarity between the ad and the query. The author further improved the model by adding `click' features (like frequency of clicks and position when the ad was clicked). For new ads, the authors proposed to leverage higher order click aggregations such as ad campaign, ad category, etc. 

Sponsored search has also been studied from the perspective of revenue maximization. It has been often modeled as an online bipartite graph where one set of the graph (the ads) is known where as the other set is coming online (the queries) \cite{mehta2007adwords, mehta2012online}. The goal of revenue maximization is to match incoming queries to existing ads to maximize the revenue. In \cite{mehta2007adwords}, the authors framed an algorithm considering the same bipartite model such that there are `n' bidders, each with a daily budget, `b' and the search engine is paid only when a user clicks on the ad. The algorithm gives a competitive ratio of $1 - 1/e$ as `b' tends to infinity and also gives $1 - 1/e$ as the lower bound. More detailed survey on further optimization on the revenue is discussed in \cite{mehta2012online} with respect to bidding assumptions, display advertising and submodular welfare maximization.

Another key issue in sponsored search is CTR prediction to improve the revenue. In \cite{graepel2010web}, the authors proposed a new online Bayesian algorithm for predicting CTR. The proposed algorithm is based on regression that maps input features to probabilities. It maintains Gaussian beliefs over weights of the model and performs Gaussian online updates derived from approximate message passing. The authors empirically show that the algorithm performs better than the considered baselines on Bing Ads. In \cite{CTREstimateNewAds}, the authors attempted to model the problem of CTR estimate for new ads as a machine learning problem. The authors selected and engineered features to train a logistic regression model. The features used in the model included \textit{term wise CTR} of the ad and CTRs of \textit{related ads}. 

Apart from the above mentioned issues, research has also been carried to prevent click frauds \cite{li2014search}, understanding bidding behaviors \cite{katona2010race} and auction design \cite{jansen2011bidding}. 
A detailed survey on sponsored search and its research issues is provided by Trimponias \textit{et al.} \cite{survey_ss}.


\section{Long tail advertising in sponsored search}
In the literature, the challenges of long tail queries are mainly addressed through query expansion or query reformulations. In this section, we review the related literature with respect to the same two approaches for addressing long tail challenges in sponsored search.

\subsection{Query Expansion}

In \cite{broder2007robust}, the authors propose a taxonomy based classification method for query classification to leverage additional information for rare queries. The authors proposed to build the taxonomy by using the indexed documents and classifying them through a centroid based method and manually labelling the centroids into a taxonomy through human annotators. The authors proposed to take the search results returned by the query as the external data to classify the query into one of the classes. The conditional probability of query, $q$, belonging to a class, $C$ in the taxonomy is the joint probability of the query belonging to the returned search result, $P(d|q)$, and probability of the search result belong to the class, $C$, $P(C|d)$.  The authors maximize the \textit{relevance} between query and first `k' organic documents to perform query classification. 


The authors in \cite{broder2008search} propose to use web relevance feedback for the expansion of queries for sponsored search. It was proposed to augment the query by three means - terms from search results of the query, external taxonomic features based on the query and lexicon features extracted from the indexed documents of the search engine. The authors proposed to extract important terms from the search results using TF-IDF, with a logarithm term frequency and IDF computed over the ad corpus. For lexicon extraction, the authors employed an inhouse tool which will identify lexicons during crawling and indexing. For taxonomy based features, the authors used a modification of their previous work \cite{broder2007robust}. The taxonomy in this paper was built using ads titles which was populated by means of human annotators. 

The authors in \cite{broder2009online} further extended the work in \cite{broder2007robust, broder2008search}. For the expansion of tail queries, the authors proposed to build an inverted index of the head queries and their features and use it as a look-up table during the query time. Query features (unigrams, phrases and taxonomy classes) were computed as stated in \cite{broder2008search}. The authors used this feature vector to do a TF-IDF which was built on the ad corpus. The authors validated their approach on 400 head queries and 400 tail queries and stated significant improvement over baselines.



Another direction to expand rare queries was taken by using the notion of feedback from the search logs \cite{song2010optimal}. The authors proposed to use the two bi-partite graphs - one graph of clicked URLs and another graph of skipped URLs for each query. The authors showed that clicked URLs and skipped URLs have similar click patterns for rare queries. The authors used the signals from the skip graph to smoothen the click graph. An optimized random walk on the click graph was framed by leveraging skip-graph to correlate the URLs for similar queries. The authors employed gradient descent to optimize the parameters of the random walk for long tail queries. The proposed model was highly scalable as it only uses the click and skip frequencies for each query and URL pair. 

\subsection{Query Reformulation}
Apart from query expansion, another approach to address the challenges of tail queries is by means of query reformulation, either in real time or at the end of search result page as query suggestions.

In \cite{zhou2012collaborative}, the authors have attempted to perform a matching from tail queries to head queries. The authors modified the relevance function, $f(q,d)$, for fetching better quality results for tail queries. The authors argued that using the notion of collaborative filtering from recommendation systems, related head queries could be potentially used to improve the ads and search results of tail queries. The authors augmented the relevance function for a document and a query for organic content retrieval by adding an extra term based on similar head queries, where similarity between two queries was measured by considering three types of features: string matching, term matching and semantic matching.. The authors minimized the risk on the augmented objective function with respect to the training data to give query reformulations for tail queries.

The authors in \cite{verma2014bringing} proposed to leverage Entity Linking in order to match head and tail queries. Using AOL search query logs, the authors extracted two sets of queries - head and tail. In the two sets, the authors tried to identify \textit{spots} (possible Entities) and then used an open source tool, Dexter to identify the entities using the word-n-grams of the search queries. The authors finally compared the overlap ratios between the head and tail partitions to understand the performance of the proposed approach. In a more recent study \cite{huang2016kb}, the authors have tried to use Knowledge Bases (KB) such as Yago \cite{YAGO} for query suggestions for long tail queries. The authors proposed to do entity linking on the search queries and use the entities to extract more knowledge from the KB. More formally, the authors attempted to integrate the Query-Flow graph (the graph which associate queries which have been seen in the same sessions) and the KB of entities. In the first step, entities are identified in the search query. In the next step, the entities are expanded by means of the KB and assigned a weight. Using the expanded entities from the second step, for each entity a Personalized Page Rank \cite{chakrabarti2007dynamic} is performed on the Query-Flow graph to fetch related queries as the query suggestions.

In another front, authors have tried to use templates to provide query recommendations for the tail queries \cite{szpektor2011improving}. The approach was motivated by the fact that most distinct queries follow similar patterns. For example, ``Los Angeles hotels'', ``New York hotels'' and ``Paris hotels'', could be generalized to ``city-name hotels''. Hence, if ``Paris restaurants'' is a query recommendation for ``Paris hotels'' and ``Chicago restaurants'' is the recommendation for ``Chicago hotels'', then based on certain confidence, a template rule can be defined as \textit{city-name hotels} $\implies$ \textit{city-name restaurants}. The authors proposed to construct such rules by means of a hierarchy over entities. The hierarchy was built using WordNet 3.0 hypernymy hierarchy and the Wikipedia category hierarchy. The authors suggest to use every unigram, bigram and trigram to be replacement candidates for the entities.

Very recently, authors in \cite{sordoni2015hierarchical} employed deep learning for query suggestions. The papers claims that while past approaches can only suggest previously seen queries as query suggestions, the proposed approach is capable of generating \textit{synthetic suggestions} as per the need. Their work is particularly important for long tail queries as these queries are seen very less and even uniquely, and hence, matching such queries to head queries is not always successful. The authors modified Recurrent Neural Networks (RNNs) into Hierarchical Recurrent Encoder-Decoder. Queries are fed word by word to an RNN and  query sessions are fed query by query parallely using another set of RNNs. The training objective of the approach is maximizing the log-likelihood of a session in predicting the next query in the session after seeing the previous ones. The proposed approach showed an increase of 5.6\% over the standard baselines on long tail queries sampled from AOL search query logs.


\section{Coverage Patterns}
Coverage patterns is the key pillar of the approaches proposed in this thesis. Hence, in this section we review the work done in coverage patterns.

The model of coverage patterns was proposed in \cite{sripada2011coverage, srinivas2012discovering}. In \cite{sripada2011coverage}, the notion of coverage patterns was proposed by defining Coverage Support (CS), minimum Relative Frequency (RF) and Overlap Ratio (OR). For a given coverage pattern $\{X_{1}, X_{2}, ... X_{n}\}$, the authors defined CS to be the fraction of transactions containing the items in the coverage pattern. OR was defined as the fraction of transactions which were common in the items contained in the coverage pattern. Minimum RF was defined to remove any items whose frequency was small enough to be ignored while mining interesting patterns. Based on the concepts defined in \cite{sripada2011coverage}, the authors in \cite{srinivas2012discovering} proposed an Apriori style algorithm to mine coverage patterns through the downward closure property of OR. The authors also incorporated bit level computations to improve the performance. The extraction of coverage patterns was further improved in \cite{srinivas2014mining} by using a pattern growth technique, Coverage
Pattern Projected Growth (CPPG) which is inspired from Frequent Pattern Projected Growth (FPPG).

The utility of coverage patterns has been discussed in \cite{kavya2016coverage}. In this study, coverage patterns have been applied to display advertising. The authors proposed to leverage click logs of a website to extract coverage patterns. The extracted coverage patterns were then used to perform an intelligent matching between guaranteed contracts of the advertisers and the web pages to optimize the publisher's revenue.

\section{How Proposed Approaches Are Different}
In this thesis, we have proposed two approaches for long tail advertising in sponsored search. Earlier approaches to address long tail query challenges in sponsored search involve either query expansion \cite{broder2007robust, broder2008search, broder2009online,song2010optimal} or query reformulation \cite{szpektor2011improving,zhou2012collaborative, verma2014bringing, huang2016kb,sordoni2015hierarchical}. In this thesis, we propose to bid on high level concepts in ad space auctions instead of individual keywords. In the first approach, we propose use a two level taxonomy for auctioning ad space in search keywords. In the second approach, we generalize it to use a multi-level taxonomy to provide more flexibility to the advertisers. Both proposed approaches argue to move beyond individual keyword bidding in search engine advertising. Compared to the earlier approaches where taxonomies or external knowledge have been used \textbf{\textit{internally}} for query expansion or query reformulations, in our approach \textbf{\textit{the taxonomy is directly exposed to the advertisers}} during the ad space auctions to target potential consumers. An advertiser is in power to chose which set of concept(s) he/she wants to bid upon.


Also, the notion of coverage patterns proposed in the literature \cite{sripada2011coverage, srinivas2014mining} is only for flat transactions. We extend this notion of coverage patterns to \textit{level-wise coverage patterns}, when a hierarchical relationship exists over the items of the transactional database.

